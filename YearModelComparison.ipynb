{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import Preprocessing as myData\n",
    "from ModelEvaluation import ModelEvaluation\n",
    "from pgmpy.readwrite import BIFReader\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison of accuracy between each years data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = BIFReader(\"model.bif\")\n",
    "model = reader.get_model(state_name_type=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Year 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: 0.9523212045169385\n",
      "roc auc score: 0.9051537070524411\n",
      "balanced_accuracy_score: 0.8103074141048823\n",
      "\n",
      "\n",
      "Binary scores:\n",
      "\n",
      "f1_score: 0.24\n",
      "precision score: 0.13953488372093023\n",
      "recall score: 0.8571428571428571\n",
      "\n",
      "\n",
      "Macro scores:\n",
      "\n",
      "recall_score_macro: 0.9051537070524411\n",
      "f1_score_macro: 0.6076943005181348\n",
      "precision_score_macro: 0.5691043118869903\n",
      "\n",
      "\n",
      "Weighted Scores:\n",
      "\n",
      "recall_score_weighted: 0.9523212045169385\n",
      "f1_score_weighted: 0.9689297300108568\n",
      "precision_score_weighted: 0.9911279784541485\n"
     ]
    }
   ],
   "source": [
    "year_1_Data = myData.importData(year = str(1))\n",
    "processeed_year_1_Data = myData.normalizeAndStandardizeData(data=year_1_Data)\n",
    "\n",
    "discrete_year_1 = myData.discretizeData(rawData=year_1_Data, processedData=processeed_year_1_Data)\n",
    "\n",
    "year1_train: pd.DataFrame\n",
    "year1_test: pd.DataFrame\n",
    "year1_test_targets: pd.Series\n",
    "year1_test_evidence: list\n",
    "year1_train, year1_test, year1_test_targets, year1_test_evidence = myData.splitData(data=discrete_year_1)\n",
    "\n",
    "year1_y_pred: list\n",
    "year1_y_true: list \n",
    "year1_y_pred, year1_y_true = ModelEvaluation.performInference(model=model,testing_evidence=year1_test_evidence,testing_targets=year1_test_targets)\n",
    "ModelEvaluation.evaluatePrint(y_pred=year1_y_pred,y_true=year1_y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Year 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: 0.9480901077375122\n",
      "roc auc score: 0.837182895757173\n",
      "balanced_accuracy_score: 0.674365791514346\n",
      "\n",
      "\n",
      "Binary scores:\n",
      "\n",
      "f1_score: 0.3291139240506329\n",
      "precision score: 0.21311475409836064\n",
      "recall score: 0.7222222222222222\n",
      "\n",
      "\n",
      "Macro scores:\n",
      "\n",
      "recall_score_macro: 0.837182895757173\n",
      "f1_score_macro: 0.6510572167374917\n",
      "precision_score_macro: 0.6039532103825136\n",
      "\n",
      "\n",
      "Weighted Scores:\n",
      "\n",
      "recall_score_weighted: 0.9480901077375122\n",
      "f1_score_weighted: 0.9616489339721204\n",
      "precision_score_weighted: 0.9810108787859325\n"
     ]
    }
   ],
   "source": [
    "\n",
    "year_2_Data = myData.importData(year = str(2))\n",
    "processeed_year_2_Data = myData.normalizeAndStandardizeData(data=year_2_Data)\n",
    "\n",
    "discrete_year_2 = myData.discretizeData(rawData=year_2_Data, processedData=processeed_year_2_Data)\n",
    "\n",
    "year2_train: pd.DataFrame\n",
    "year2_test: pd.DataFrame\n",
    "year2_test_targets: pd.Series\n",
    "year2_test_evidence: list\n",
    "year2_train, year2_test, year2_test_targets, year2_test_evidence = myData.splitData(data=discrete_year_2)\n",
    "\n",
    "year2_y_pred: list\n",
    "year2_y_true: list \n",
    "year2_y_pred, year2_y_true = ModelEvaluation.performInference(model=model,testing_evidence=year2_test_evidence,testing_targets=year2_test_targets)\n",
    "ModelEvaluation.evaluatePrint(y_pred=year2_y_pred,y_true=year2_y_true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Year 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: 0.9565573770491803\n",
      "roc auc score: 0.8461216338100761\n",
      "balanced_accuracy_score: 0.692243267620152\n",
      "\n",
      "\n",
      "Binary scores:\n",
      "\n",
      "f1_score: 0.4175824175824176\n",
      "precision score: 0.2923076923076923\n",
      "recall score: 0.7307692307692307\n",
      "\n",
      "\n",
      "Macro scores:\n",
      "\n",
      "recall_score_macro: 0.846121633810076\n",
      "f1_score_macro: 0.6975098124523412\n",
      "precision_score_macro: 0.6431235431235431\n",
      "\n",
      "\n",
      "Weighted Scores:\n",
      "\n",
      "recall_score_weighted: 0.9565573770491803\n",
      "f1_score_weighted: 0.9655058757376451\n",
      "precision_score_weighted: 0.9789865871833084\n"
     ]
    }
   ],
   "source": [
    "\n",
    "year_3_Data = myData.importData(year = str(3))\n",
    "processeed_year_3_Data = myData.normalizeAndStandardizeData(data=year_3_Data)\n",
    "\n",
    "discrete_year_3 = myData.discretizeData(rawData=year_3_Data, processedData=processeed_year_3_Data)\n",
    "\n",
    "year3_train: pd.DataFrame\n",
    "year3_test: pd.DataFrame\n",
    "year3_test_targets: pd.Series\n",
    "year3_test_evidence: list\n",
    "year3_train, year3_test, year3_test_targets, year3_test_evidence = myData.splitData(data=discrete_year_3)\n",
    "\n",
    "year3_y_pred: list\n",
    "year3_y_true: list \n",
    "year3_y_pred, year3_y_true = ModelEvaluation.performInference(model=model,testing_evidence=year3_test_evidence,testing_targets=year3_test_targets)\n",
    "ModelEvaluation.evaluatePrint(y_pred=year3_y_pred,y_true=year3_y_true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Year 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: 0.9706129303106633\n",
      "roc auc score: 0.5310404178289513\n",
      "balanced_accuracy_score: 0.06208083565790257\n",
      "\n",
      "\n",
      "Binary scores:\n",
      "\n",
      "f1_score: 0.10256410256410256\n",
      "precision score: 0.2\n",
      "recall score: 0.06896551724137931\n",
      "\n",
      "\n",
      "Macro scores:\n",
      "\n",
      "recall_score_macro: 0.5310404178289513\n",
      "f1_score_macro: 0.5438129945172199\n",
      "precision_score_macro: 0.5885690093141406\n",
      "\n",
      "\n",
      "Weighted Scores:\n",
      "\n",
      "recall_score_weighted: 0.9706129303106633\n",
      "f1_score_weighted: 0.9635736952585143\n",
      "precision_score_weighted: 0.9582152625071894\n"
     ]
    }
   ],
   "source": [
    "\n",
    "year_4_Data = myData.importData(year = str(4))\n",
    "processeed_year_4_Data = myData.normalizeAndStandardizeData(data=year_4_Data)\n",
    "\n",
    "discrete_year_4 = myData.discretizeData(rawData=year_4_Data, processedData=processeed_year_4_Data)\n",
    "\n",
    "year4_train: pd.DataFrame\n",
    "year4_test: pd.DataFrame\n",
    "year4_test_targets: pd.Series\n",
    "year4_test_evidence: list\n",
    "year4_train, year4_test, year4_test_targets, year4_test_evidence = myData.splitData(data=discrete_year_4)\n",
    "\n",
    "year4_y_pred: list\n",
    "year4_y_true: list \n",
    "year4_y_pred, year4_y_true = ModelEvaluation.performInference(model=model,testing_evidence=year4_test_evidence,testing_targets=year4_test_targets)\n",
    "ModelEvaluation.evaluatePrint(y_pred=year4_y_pred,y_true=year4_y_true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Year 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: 0.9722589167767504\n",
      "roc auc score: 0.8890710382513661\n",
      "balanced_accuracy_score: 0.7781420765027323\n",
      "\n",
      "\n",
      "Binary scores:\n",
      "\n",
      "f1_score: 0.6557377049180327\n",
      "precision score: 0.5555555555555556\n",
      "recall score: 0.8\n",
      "\n",
      "\n",
      "Macro scores:\n",
      "\n",
      "recall_score_macro: 0.8890710382513661\n",
      "f1_score_macro: 0.8206424243791816\n",
      "precision_score_macro: 0.77431037139775\n",
      "\n",
      "\n",
      "Weighted Scores:\n",
      "\n",
      "recall_score_weighted: 0.9722589167767504\n",
      "f1_score_weighted: 0.9746551544439532\n",
      "precision_score_weighted: 0.9786163883071709\n"
     ]
    }
   ],
   "source": [
    "\n",
    "year_5_Data = myData.importData(year = str(5))\n",
    "processeed_year_5_Data = myData.normalizeAndStandardizeData(data=year_5_Data)\n",
    "\n",
    "discrete_year_5 = myData.discretizeData(rawData=year_5_Data, processedData=processeed_year_5_Data)\n",
    "\n",
    "year5_train: pd.DataFrame\n",
    "year5_test: pd.DataFrame\n",
    "year5_test_targets: pd.Series\n",
    "year5_test_evidence: list\n",
    "year5_train, year5_test, year5_test_targets, year5_test_evidence = myData.splitData(data=discrete_year_5)\n",
    "\n",
    "year5_y_pred: list\n",
    "year5_y_true: list \n",
    "year5_y_pred, year5_y_true = ModelEvaluation.performInference(model=model,testing_evidence=year5_test_evidence,testing_targets=year5_test_targets)\n",
    "ModelEvaluation.evaluatePrint(y_pred=year5_y_pred,y_true=year5_y_true)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
