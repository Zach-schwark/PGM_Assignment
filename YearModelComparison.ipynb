{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zachs\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from preprocessing import Preprocessing as myData\n",
    "from ModelEvaluation import ModelEvaluation\n",
    "from pgmpy.readwrite import BIFReader\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison of accuracy between each years data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = BIFReader(\"model.bif\")\n",
    "model = reader.get_model(state_name_type=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Year 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: 0.9510664993726474\n",
      "roc auc score: 0.9045207956600362\n",
      "balanced_accuracy_score: 0.8090415913200721\n",
      "\n",
      "\n",
      "Binary scores:\n",
      "\n",
      "f1_score: 0.23529411764705882\n",
      "precision score: 0.13636363636363635\n",
      "recall score: 0.8571428571428571\n",
      "\n",
      "\n",
      "Macro scores:\n",
      "\n",
      "recall_score_macro: 0.9045207956600361\n",
      "f1_score_macro: 0.6050093400937822\n",
      "precision_score_macro: 0.5675178075576481\n",
      "\n",
      "\n",
      "Weighted Scores:\n",
      "\n",
      "recall_score_weighted: 0.9510664993726474\n",
      "f1_score_weighted: 0.9682301922591328\n",
      "precision_score_weighted: 0.9910983797595444\n"
     ]
    }
   ],
   "source": [
    "year_1_Data = myData.importData(year = str(1))\n",
    "processeed_year_1_Data = myData.normalizeAndStandardizeData(data=year_1_Data)\n",
    "\n",
    "discrete_year_1 = myData.discretizeData(rawData=year_1_Data, processedData=processeed_year_1_Data)\n",
    "\n",
    "year1_train: pd.DataFrame\n",
    "year1_test: pd.DataFrame\n",
    "year1_test_targets: pd.Series\n",
    "year1_test_evidence: list\n",
    "year1_train, year1_test, year1_test_targets, year1_test_evidence = myData.splitData(data=discrete_year_1)\n",
    "\n",
    "year1_y_pred: list\n",
    "year1_y_true: list \n",
    "year1_y_pred, year1_y_true = ModelEvaluation.performInference(model=model,testing_evidence=year1_test_evidence,testing_targets=year1_test_targets)\n",
    "ModelEvaluation.evaluate(y_pred=year1_y_pred,y_true=year1_y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Year 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: 0.9363369245837414\n",
      "roc auc score: 0.8584801152099257\n",
      "balanced_accuracy_score: 0.7169602304198515\n",
      "\n",
      "\n",
      "Binary scores:\n",
      "\n",
      "f1_score: 0.3010752688172043\n",
      "precision score: 0.18666666666666668\n",
      "recall score: 0.7777777777777778\n",
      "\n",
      "\n",
      "Macro scores:\n",
      "\n",
      "recall_score_macro: 0.8584801152099257\n",
      "f1_score_macro: 0.6338624163480583\n",
      "precision_score_macro: 0.5912191684284708\n",
      "\n",
      "\n",
      "Weighted Scores:\n",
      "\n",
      "recall_score_weighted: 0.9363369245837414\n",
      "f1_score_weighted: 0.954915638990459\n",
      "precision_score_weighted: 0.9815073312447068\n"
     ]
    }
   ],
   "source": [
    "\n",
    "year_2_Data = myData.importData(year = str(2))\n",
    "processeed_year_2_Data = myData.normalizeAndStandardizeData(data=year_2_Data)\n",
    "\n",
    "discrete_year_2 = myData.discretizeData(rawData=year_2_Data, processedData=processeed_year_2_Data)\n",
    "\n",
    "year2_train: pd.DataFrame\n",
    "year2_test: pd.DataFrame\n",
    "year2_test_targets: pd.Series\n",
    "year2_test_evidence: list\n",
    "year2_train, year2_test, year2_test_targets, year2_test_evidence = myData.splitData(data=discrete_year_2)\n",
    "\n",
    "year2_y_pred: list\n",
    "year2_y_true: list \n",
    "year2_y_pred, year2_y_true = ModelEvaluation.performInference(model=model,testing_evidence=year2_test_evidence,testing_targets=year2_test_targets)\n",
    "ModelEvaluation.evaluate(y_pred=year2_y_pred,y_true=year2_y_true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Year 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: 0.9483606557377049\n",
      "roc auc score: 0.9171820641669888\n",
      "balanced_accuracy_score: 0.8343641283339776\n",
      "\n",
      "\n",
      "Binary scores:\n",
      "\n",
      "f1_score: 0.42201834862385323\n",
      "precision score: 0.27710843373493976\n",
      "recall score: 0.8846153846153846\n",
      "\n",
      "\n",
      "Macro scores:\n",
      "\n",
      "recall_score_macro: 0.9171820641669888\n",
      "f1_score_macro: 0.6974956607984131\n",
      "precision_score_macro: 0.6372349556537495\n",
      "\n",
      "\n",
      "Weighted Scores:\n",
      "\n",
      "recall_score_weighted: 0.9483606557377049\n",
      "f1_score_weighted: 0.9612313170442213\n",
      "precision_score_weighted: 0.9820118225399544\n"
     ]
    }
   ],
   "source": [
    "\n",
    "year_3_Data = myData.importData(year = str(3))\n",
    "processeed_year_3_Data = myData.normalizeAndStandardizeData(data=year_3_Data)\n",
    "\n",
    "discrete_year_3 = myData.discretizeData(rawData=year_3_Data, processedData=processeed_year_3_Data)\n",
    "\n",
    "year3_train: pd.DataFrame\n",
    "year3_test: pd.DataFrame\n",
    "year3_test_targets: pd.Series\n",
    "year3_test_evidence: list\n",
    "year3_train, year3_test, year3_test_targets, year3_test_evidence = myData.splitData(data=discrete_year_3)\n",
    "\n",
    "year3_y_pred: list\n",
    "year3_y_true: list \n",
    "year3_y_pred, year3_y_true = ModelEvaluation.performInference(model=model,testing_evidence=year3_test_evidence,testing_targets=year3_test_targets)\n",
    "ModelEvaluation.evaluate(y_pred=year3_y_pred,y_true=year3_y_true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Year 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: 0.963056255247691\n",
      "roc auc score: 0.6616564781292659\n",
      "balanced_accuracy_score: 0.32331295625853174\n",
      "\n",
      "\n",
      "Binary scores:\n",
      "\n",
      "f1_score: 0.3125\n",
      "precision score: 0.2857142857142857\n",
      "recall score: 0.3448275862068966\n",
      "\n",
      "\n",
      "Macro scores:\n",
      "\n",
      "recall_score_macro: 0.6616564781292659\n",
      "f1_score_macro: 0.6467590595340811\n",
      "precision_score_macro: 0.6346391497775581\n",
      "\n",
      "\n",
      "Weighted Scores:\n",
      "\n",
      "recall_score_weighted: 0.963056255247691\n",
      "f1_score_weighted: 0.96474017998086\n",
      "precision_score_weighted: 0.966571871006515\n"
     ]
    }
   ],
   "source": [
    "\n",
    "year_4_Data = myData.importData(year = str(4))\n",
    "processeed_year_4_Data = myData.normalizeAndStandardizeData(data=year_4_Data)\n",
    "\n",
    "discrete_year_4 = myData.discretizeData(rawData=year_4_Data, processedData=processeed_year_4_Data)\n",
    "\n",
    "year4_train: pd.DataFrame\n",
    "year4_test: pd.DataFrame\n",
    "year4_test_targets: pd.Series\n",
    "year4_test_evidence: list\n",
    "year4_train, year4_test, year4_test_targets, year4_test_evidence = myData.splitData(data=discrete_year_4)\n",
    "\n",
    "year4_y_pred: list\n",
    "year4_y_true: list \n",
    "year4_y_pred, year4_y_true = ModelEvaluation.performInference(model=model,testing_evidence=year4_test_evidence,testing_targets=year4_test_targets)\n",
    "ModelEvaluation.evaluate(y_pred=year4_y_pred,y_true=year4_y_true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Year 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: 0.9709379128137384\n",
      "roc auc score: 0.9077049180327869\n",
      "balanced_accuracy_score: 0.8154098360655737\n",
      "\n",
      "\n",
      "Binary scores:\n",
      "\n",
      "f1_score: 0.65625\n",
      "precision score: 0.5384615384615384\n",
      "recall score: 0.84\n",
      "\n",
      "\n",
      "Macro scores:\n",
      "\n",
      "recall_score_macro: 0.9077049180327869\n",
      "f1_score_macro: 0.8205387931034482\n",
      "precision_score_macro: 0.7664452539104349\n",
      "\n",
      "\n",
      "Weighted Scores:\n",
      "\n",
      "recall_score_weighted: 0.9709379128137384\n",
      "f1_score_weighted: 0.9739762788684918\n",
      "precision_score_weighted: 0.979370599778823\n"
     ]
    }
   ],
   "source": [
    "\n",
    "year_5_Data = myData.importData(year = str(5))\n",
    "processeed_year_5_Data = myData.normalizeAndStandardizeData(data=year_5_Data)\n",
    "\n",
    "discrete_year_5 = myData.discretizeData(rawData=year_5_Data, processedData=processeed_year_5_Data)\n",
    "\n",
    "year5_train: pd.DataFrame\n",
    "year5_test: pd.DataFrame\n",
    "year5_test_targets: pd.Series\n",
    "year5_test_evidence: list\n",
    "year5_train, year5_test, year5_test_targets, year5_test_evidence = myData.splitData(data=discrete_year_5)\n",
    "\n",
    "year5_y_pred: list\n",
    "year5_y_true: list \n",
    "year5_y_pred, year5_y_true = ModelEvaluation.performInference(model=model,testing_evidence=year5_test_evidence,testing_targets=year5_test_targets)\n",
    "ModelEvaluation.evaluate(y_pred=year5_y_pred,y_true=year5_y_true)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
